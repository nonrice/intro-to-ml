{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diamond Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to download the dataset! You can find it linked inside of the slideshow (found in Google Classroom). When you download it, make sure to move it into the same folder as the main notebook. You don't need to worry about all these fancy headers I've added to this notebook. Just focus on the code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry if these imports do not work. We will get them sorted next class.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will load the dataset into our program. Below, we are just printing a few things about it, such as size.\n",
    "df = pd.read_csv(\"/kaggle/input/diamonds/diamonds.csv\", index_col=0)",
    "\n",
    "print(\"dataset size:\", len(df))\n",
    "print(df.head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These three things are called \"maps\". You give them an input, they give you the output.\n",
    "# Notice how some of the diamond qualities are described using words. We can convert them into numbers based on what they mean.\n",
    "cut_enc = {\n",
    "    \"Fair\": 0,\n",
    "    \"Good\": 1,\n",
    "    \"Very Good\": 2,\n",
    "    \"Premium\": 3,\n",
    "    \"Ideal\": 4\n",
    "}\n",
    "\n",
    "clarity_enc = {\n",
    "    \"I1\": 0, # Worst Clarity\n",
    "    \"SI2\": 1,\n",
    "    \"SI1\": 2,\n",
    "    \"VS2\": 3,\n",
    "    \"VS1\": 4,\n",
    "    \"VVS2\": 5,\n",
    "    \"VVS1\": 6,\n",
    "    \"IF\": 7, # Best Clarity\n",
    "}\n",
    "\n",
    "color_enc = {\n",
    "    \"J\": 0, # Worst Color\n",
    "    \"I\": 1,\n",
    "    \"H\": 2,\n",
    "    \"G\": 3,\n",
    "    \"F\": 4,\n",
    "    \"E\": 5,\n",
    "    \"D\": 6, # Best Color\n",
    "}\n",
    "\n",
    "# These lines will take the entire dataset, and replace the words using our maps that we made.\n",
    "df[\"cut\"] = df[\"cut\"].map(cut_enc)\n",
    "df[\"clarity\"] = df[\"clarity\"].map(clarity_enc)\n",
    "df[\"color\"] = df[\"color\"].map(color_enc)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a correlation heatmap. This should look like a grid with different colors.\n",
    "# Each cell represents the correlation between the feature in the cell's column, and the feature in the cell's row.\n",
    "# High correlation means they are more closely related.\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "\n",
    "# note that X, Y, and Z have a high correlation with Carats (makes sense, weight is calculated from dimensions)\n",
    "# as a result, we are able to remove them without losing much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation (Cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Split into input and output data. These are our \"test questions\" used to train the neural network.\n",
    "# X will store the \"test questions\", and y will store the \"answer key\"\n",
    "X = df.drop([\"price\", \"x\", \"y\", \"z\"], axis=1)\n",
    "y = df[\"price\"].values\n",
    "y = np.reshape(y, (-1, 1)) # This reshapes y from an array (wrong) to an array of rows with size 1 (right)\n",
    "\n",
    "# Some visualization stuff\n",
    "print(X.head())\n",
    "print(X.shape)\n",
    "print(\"---\")\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Lets split our test questions into two parts.\n",
    "# The first part, X_train and y_train will be used to train our model.\n",
    "# The second part will act as the \"new data\" our model has never seen before.\n",
    "# By testing the model on the second part, but not letting it learn from it, it might as well be \"new data\". This way, we can detect overfitting!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "# Scaling the data into the range 0...1 for better performance in neural networks\n",
    "X_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "print(X_train)\n",
    "print(\"---\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# In tensorflow, you build neural networks layer by layer. Very simple.\n",
    "# Here, we add a two layers of 64 neurons. Our output layer contains a single neuron, which represents the predicted price of the input diamond.\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"relu\", input_dim=6))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Finishing up our model. There is nothing much below this that you need to take note of.\n",
    "from tensorflow.keras.losses import mae\n",
    "model.compile(loss=mae, optimizer=\"Adam\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will begin the training process!\n",
    "# \"epochs\" represents the number of times we will go through all of our test questions in \"X_train\".\n",
    "# As you can see, we will test our neural network on these questions for 64 times.\n",
    "model_stats = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=64,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the model loss as a graph to see how good it is\n",
    "# Our goal is to see the loss go down. Loss measures how bad a neural network performs.\n",
    "plt.plot(model_stats.history[\"loss\"])\n",
    "plt.plot(model_stats.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"test loss\"], loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now supply our own data to see what the neural network gives us.\n",
    "# Remember, we need to preprocess it in the same way we did with the training data!\n",
    "X_pred = [1, \"Ideal\", \"I\", \"VS1\", 63.6, 57.0]\n",
    "\n",
    "X_pred[1] = cut_enc[X_pred[1]]\n",
    "X_pred[2] = color_enc[X_pred[2]]\n",
    "X_pred[3] = clarity_enc[X_pred[3]]\n",
    "\n",
    "X_pred = np.reshape(X_pred, (1, -1))\n",
    "X_pred = X_scaler.transform(X_pred)\n",
    "\n",
    "print(X_pred)\n",
    "\n",
    "y_pred = model.predict(X_pred)\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Predicted price:\", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dfbd2e278a2b04bc7e57a86c2885905c8d8a47fbdd48769f3daa871fa8c4e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
